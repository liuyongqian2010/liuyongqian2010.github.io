<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>LiuYongQian</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="永远相信美好的事情将要发生！">
<meta property="og:type" content="website">
<meta property="og:title" content="LiuYongQian">
<meta property="og:url" content="http://yoursite.com/page/6/index.html">
<meta property="og:site_name" content="LiuYongQian">
<meta property="og:description" content="永远相信美好的事情将要发生！">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LiuYongQian">
<meta name="twitter:description" content="永远相信美好的事情将要发生！">
  
    <link rel="alternate" href="/atom.xml" title="LiuYongQian" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">LiuYongQian</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">永远相信美好的事情将要发生！</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-大数据的一些函数-02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/24/大数据的一些函数-02/" class="article-date">
  <time datetime="2018-04-24T14:51:09.000Z" itemprop="datePublished">2018-04-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/24/大数据的一些函数-02/">大数据的一些函数-02</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>大数据的一些函数-02</h5><p></p>
<hr>
<hr>
<ul>
<li><p>parallelize</p>
<pre><code>from pyspark import SparkContext,SparkConf

conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)
#用 python list 初始化
data =[1,2,3,4,5,6,7,8,9]
rdd = sc.parallelize(data,numSlices=4)       #parallelize并行化数据，转化为RDD;numSlices为分块数目，根据集群数进行分块
print(rdd.collect())             #collect返回RDD所有元素
print(rdd.getNumPartitions())    #返回分块数
print(rdd.glom().collect())      #glom，说明rdd中的元素已经变成了分片映射的列表
print(rdd.first())
sc.stop()
---------------------------------
[1, 2, 3, 4, 5, 6, 7, 8, 9]
4
[[1, 2], [3, 4], [5, 6], [7, 8, 9]]
1
</code></pre></li>
<li><p>读取一个文本文件</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)

#读取一个文本文件
rdd = sc.textFile(&quot;file:///home/hadoop/data1.txt&quot;)
print(rdd.collect())
print(rdd.getNumPartitions())
print(rdd.glom().collect())
#执行Map运算
rdda=rdd.map(lambda x:len(x))
print(rdda.collect())
sc.stop()
-----------------------------------
[&apos;Hello, Spark&apos;, &apos;textfile&apos;, &apos;end&apos;]
2
[[&apos;Hello, Spark&apos;], [&apos;textfile&apos;, &apos;end&apos;]]
[12, 8, 3]
</code></pre></li>
<li><p>读取所有文本文件</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)

#读取所有文本文件
rdd = sc.wholeTextFiles(&quot;file:///Users/chuzhengkai/Desktop/*.txt&quot;)
a=rdd.collect()
for b in a:
    #b是一个元组(文件名,内容)
    print(b[0])
sc.stop()
-------------------------------------
file:/Users/chuzhengkai/Desktop/test0.txt
file:/Users/chuzhengkai/Desktop/test.txt
</code></pre></li>
<li><p>map,filter</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)
#用 python list 初始化
data =[x for x in range(10)]
rdd = sc.parallelize(data)
print(rdd.collect())
def pf(x):
    return x**2
def ou(x):
    return x%2==0
#map运算
xrdd = rdd.map(pf)
print(xrdd.collect())
#filter运算
xrdd = rdd.map(pf).filter(ou)
print(xrdd.collect())
sc.stop()
-------------------------------------------
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
[0, 4, 16, 36, 64]
</code></pre></li>
<li><p>map,flatmap</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)
#用 python list 初始化
data =[&apos;Hello guys&apos;,&apos;My name is David&apos;,&apos;Welcome to USA&apos;]
rdd = sc.parallelize(data)
print(rdd.collect())
def fg(x):
    return x.split(&apos; &apos;)

#map操作
xrdd = rdd.map(fg)
print(xrdd.collect())

#flatMap
xrdd = rdd.flatMap(fg)
print(xrdd.collect())
sc.stop()
-----------------------------------------
[&apos;Hello guys&apos;, &apos;My name is David&apos;, &apos;Welcome to USA&apos;]
[[&apos;Hello&apos;, &apos;guys&apos;], [&apos;My&apos;, &apos;name&apos;, &apos;is&apos;, &apos;David&apos;], [&apos;Welcome&apos;, &apos;to&apos;, &apos;USA&apos;]]
[&apos;Hello&apos;, &apos;guys&apos;, &apos;My&apos;, &apos;name&apos;, &apos;is&apos;, &apos;David&apos;, &apos;Welcome&apos;, &apos;to&apos;, &apos;USA&apos;]
</code></pre></li>
<li><p>词频统计</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)
#读取一个文本文件
rdd = sc.textFile(&quot;file:///Users/chuzhengkai/Desktop/test0.txt&quot;)
print(rdd.collect())
def wmap(x):
    return (x,1)

def wreduce(x,y):
    return x+y

#执行Map运算
rddm=rdd.map(wmap)
print(rddm.collect())
#执行reduce运算
rddr=rddm.reduceByKey(wreduce)
print(rddr.collect())
sc.stop()
------------------------------------------------------
[&apos;good&apos;, &apos;day&apos;, &apos;has&apos;, &apos;good&apos;, &apos;weather&apos;, &apos;today&apos;, &apos;has&apos;, &apos;good&apos;, &apos;weather&apos;, &apos;today&apos;, &apos;is&apos;, &apos;good&apos;, &apos;day&apos;]
[(&apos;good&apos;, 1), (&apos;day&apos;, 1), (&apos;has&apos;, 1), (&apos;good&apos;, 1), (&apos;weather&apos;, 1), (&apos;today&apos;, 1), (&apos;has&apos;, 1), (&apos;good&apos;, 1), (&apos;weather&apos;, 1), (&apos;today&apos;, 1), (&apos;is&apos;, 1), (&apos;good&apos;, 1), (&apos;day&apos;, 1)]
[(&apos;good&apos;, 4), (&apos;weather&apos;, 2), (&apos;is&apos;, 1), (&apos;day&apos;, 2), (&apos;has&apos;, 2), (&apos;today&apos;, 2)]
</code></pre></li>
<li><p>去重复</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)

#读取一个文本文件
rdd = sc.textFile(&quot;file:///Users/chuzhengkai/Desktop/test0.txt&quot;)
print(rdd.collect())
print(rdd.distinct().collect())
sc.stop()
--------------------------------------------------
[&apos;good&apos;, &apos;day&apos;, &apos;has&apos;, &apos;good&apos;, &apos;weather&apos;, &apos;today&apos;, &apos;has&apos;, &apos;good&apos;, &apos;weather&apos;, &apos;today&apos;, &apos;is&apos;, &apos;good&apos;, &apos;day&apos;]
[&apos;good&apos;, &apos;weather&apos;, &apos;is&apos;, &apos;day&apos;, &apos;has&apos;, &apos;today&apos;]
</code></pre></li>
<li><p>排序</p>
<pre><code>conf=SparkConf().setAppName(&apos;demoPrj&apos;).setMaster(&apos;local[*]&apos;)
sc = SparkContext.getOrCreate(conf)

#读取一个文本文件
rdd = sc.textFile(&quot;file:///Users/chuzhengkai/Desktop/test0.txt&quot;)
print(rdd.collect())
def wmap(x):
    return (x,1)
def wreduce(x,y):
    return x+y
def px(x):
    return x[1]
#执行Map运算
rddm=rdd.map(wmap)
print(rddm.collect())
#执行reduce运算
rddr=rddm.reduceByKey(wreduce)
print(rddr.collect())
#对结果执行按词频排序
rdds=rddr.sortBy(px,ascending=False)
print(rdds.collectAsMap())
#取前三名
print(rdds.take(3))
#按key排序
print(rddr.sortByKey().collect())
#按key分组
print(rddm.groupByKey().map(lambda x:{x[0]:[y for y in x[1]]}).collect())
sc.stop()
----------------------------------------------------------
[&apos;good&apos;, &apos;day&apos;, &apos;has&apos;, &apos;good&apos;, &apos;weather&apos;, &apos;today&apos;, &apos;has&apos;, &apos;good&apos;, &apos;weather&apos;, &apos;today&apos;, &apos;is&apos;, &apos;good&apos;, &apos;day&apos;]
[(&apos;good&apos;, 1), (&apos;day&apos;, 1), (&apos;has&apos;, 1), (&apos;good&apos;, 1), (&apos;weather&apos;, 1), (&apos;today&apos;, 1), (&apos;has&apos;, 1), (&apos;good&apos;, 1), (&apos;weather&apos;, 1), (&apos;today&apos;, 1), (&apos;is&apos;, 1), (&apos;good&apos;, 1), (&apos;day&apos;, 1)]
[(&apos;good&apos;, 4), (&apos;weather&apos;, 2), (&apos;is&apos;, 1), (&apos;day&apos;, 2), (&apos;has&apos;, 2), (&apos;today&apos;, 2)]
{&apos;good&apos;: 4, &apos;weather&apos;: 2, &apos;day&apos;: 2, &apos;has&apos;: 2, &apos;today&apos;: 2, &apos;is&apos;: 1}
[(&apos;good&apos;, 4), (&apos;weather&apos;, 2), (&apos;day&apos;, 2)]
[(&apos;day&apos;, 2), (&apos;good&apos;, 4), (&apos;has&apos;, 2), (&apos;is&apos;, 1), (&apos;today&apos;, 2), (&apos;weather&apos;, 2)]
[{&apos;good&apos;: [1, 1, 1, 1]}, {&apos;weather&apos;: [1, 1]}, {&apos;is&apos;: [1]}, {&apos;day&apos;: [1, 1]}, {&apos;has&apos;: [1, 1]}, {&apos;today&apos;: [1, 1]}]
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/24/大数据的一些函数-02/" data-id="ck2ormwf9006ci4wvux2dyhbo" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-大数据的一些函数-01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/24/大数据的一些函数-01/" class="article-date">
  <time datetime="2018-04-24T12:41:20.000Z" itemprop="datePublished">2018-04-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/24/大数据的一些函数-01/">大数据的一些函数-01</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>大数据一些函数-01</h5><p></p>
<hr>
<hr>
<p></p><h6>准备工作</h6><p></p>
<ul>
<li>1.已经安装配置完毕如下三大件：<ul>
<li>hadoop-2.9.0</li>
<li>python3.6.3</li>
<li>spark2.3.0（可直接pip install pyspark）</li>
</ul>
</li>
<li>2.启动工作：<ul>
<li>start-all.sh 启动hadoop服务</li>
<li>jupyter-notebook –ip python333后使用提示的url打开jupyter页面（python333为我的主机名）</li>
<li>pyspark启动spark</li>
<li>之后就在jupyter中进行操作了</li>
</ul>
</li>
</ul>
<p></p><h6>一些函数</h6><p></p>
<ul>
<li><p>创建会话</p>
<pre><code>#导入模块
import pyspark
#导入类
from pyspark import SparkContext,SparkConf
#创建配置,指定AppName,指定Master(主机)
conf = SparkConf().setAppName(&apos;demoRDD&apos;).setMaster(&apos;local[*]&apos;)
#创建会话
sc = SparkContext.getOrCreate(conf)
#通过会话实现对SPARK的操作
#以python list提供一个测试用的数据
data = [x for x in range(11)]
rdd = sc.parallelize(data)
print(&apos;RDD对象:&apos;,rdd,&apos;\n记录数:&apos;,rdd.count())
partiRdd = rdd.glom()
print(&apos;RDD.GLOM Collect结果:&apos;,partiRdd.collect())
print(partiRdd.count())
print(&apos;RDD Collect结果:&apos;,rdd.collect())
#关闭会话
sc.stop()
--------------------------------------------------------
RDD对象: ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 
记录数: 11
RDD.GLOM Collect结果: [[0, 1], [2, 3, 4, 5], [6, 7], [8, 9, 10]]
4
RDD Collect结果: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre></li>
<li><p>map、filter操作</p>
<pre><code>#创建会话
sc = SparkContext.getOrCreate(conf)
#通过会话实现对SPARK的操作
#以python list提供一个测试用的数据
data = [x for x in range(11)]
rdd = sc.parallelize(data)
print(&apos;RDD对象记录数:&apos;,rdd.count())
print(&apos;RDD Collect结果:&apos;,rdd.collect())
#map操作,映射
mappedRdd = rdd.map(lambda x:x**3)
print(mappedRdd.collect())
#判断一个数是否是偶数
def filterOdd(x):
    return x%2 == 0
#过滤操作
filtedRdd = mappedRdd.filter(filterOdd)
print(filtedRdd.collect())
print(rdd.collect())
#关闭会话
sc.stop()
-----------------------------------------------
RDD对象记录数: 11
RDD Collect结果:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
[0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]
[0, 8, 64, 216, 512, 1000]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre></li>
<li><p>使用文本文件做数据源</p>
<pre><code>sc = SparkContext.getOrCreate(conf)
rows = sc.textFile(&quot;file:///home/hadoop/data1.txt&quot;)
print(rows.first())
print(rows.take(2))
print(rows.count())
print(rows.top(2))
sc.stop()
</code></pre></li>
<li><p>使用多个文本文件，进行词频统计</p>
<pre><code>sc = SparkContext.getOrCreate(conf)
#多个文本文件获取到一个RDD里面
filesRDD = sc.wholeTextFiles(&apos;file:///home/hadoop/*.txt&apos;)
#文件内容RDD
fileConRDD = filesRDD.map(lambda x:x[1])
#用回车符分隔字符串,形成列表
def sp(x):
    return x.split(&apos;\n&apos;)
#对每个文件内容做映射,结果是多个文件内容列表
#存在二维结构
strRDD = fileConRDD.map(sp)
#同样是映射,结果展平成一维结构
wordRDD = fileConRDD.flatMap(sp)
#结果,形成类一个元组表达一个文件,多个元组的列表
#词频统计map
wordDictRDD = wordRDD.map(lambda x:(x,1))
#Reduce
r = wordDictRDD.reduceByKey(lambda x,y:x+y)
print(strRDD.collect())
print(wordRDD.collect())
print(wordDictRDD.collect())
print(r.collect())
sc.stop()
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/24/大数据的一些函数-01/" data-id="ck2ormwf00068i4wv7w7epxm1" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-Centos7安装MySQL5.6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/16/Centos7安装MySQL5.6/" class="article-date">
  <time datetime="2018-04-16T15:41:40.000Z" itemprop="datePublished">2018-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/16/Centos7安装MySQL5.6/">Centos7安装MySQL5.6</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>Centos7安装MySQL5.6</h5><p></p>
<hr>
<hr>
<p></p><h6>安装MySQL</h6><p></p>
<ul>
<li>1.下载安装包mysql-5.6.40-linux-glibc2.12-x86_64.tar.gz</li>
<li><p>2.卸载centos7自带的Mariadb数据库</p>
<pre><code>[root@localhost ~]# rpm -qa|grep mariadb  // 查询出来已安装的mariadb  
[root@localhost ~]# rpm -e --nodeps 文件名  // 卸载mariadb，文件名为上述命令查询出来的文件
[root@localhost ~]# rm /etc/my.cnf    //删除etc目录下的my.cnf
</code></pre></li>
<li><p>3.创建一个用户名为mysql的用户并加入mysql用户组</p>
<pre><code>[root@localhost ~]# groupadd mysql  
[root@localhost ~]# useradd -g mysql mysql  
</code></pre></li>
<li><p>4.将下载的压缩包放到 /usr/local/ 目录下</p>
</li>
<li><p>5.解压安装包</p>
<pre><code>[root@localhost local]# tar -zxvf mysql-5.6.40-linux-glibc2.12-x86_64.tar.gz
</code></pre></li>
<li><p>6.将解压好的文件夹重命名为mysql</p>
<pre><code>[root@localhost local]# mv 解压出来的文件夹名 mysql
</code></pre></li>
<li><p>7.在/etc/下新建配置文件my.cnf，并在该文件中添加以下代码：</p>
<pre><code>[mysql]  
# 设置mysql客户端默认字符集  
default-character-set=utf8   
socket=/var/lib/mysql/mysql.sock  

[mysqld]  
skip-name-resolve  
#设置3306端口  
port = 3306   
socket=/var/lib/mysql/mysql.sock  
# 设置mysql的安装目录  
basedir=/usr/local/mysql  
# 设置mysql数据库的数据的存放目录  
datadir=/usr/local/mysql/data  
# 允许最大连接数  
max_connections=200  
# 服务端使用的字符集默认为8比特编码的latin1字符集  
character-set-server=utf8  
# 创建新表时将使用的默认存储引擎  
default-storage-engine=INNODB  
lower_case_table_name=1  
max_allowed_packet=16M  
</code></pre></li>
<li><p>8.进入安装mysql软件目录</p>
<pre><code>[root@localhost mysql]# chown -R mysql:mysql ./       修改当前目录拥有着为mysql用户  
[root@localhost mysql]# ./scripts/mysql_install_db --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/           安装数据库
</code></pre></li>
</ul>
<p></p><h6>配置MySQL</h6>  <p></p>
<ul>
<li><p>1.授予my.cnf最大权限</p>
<pre><code>[root@localhost ~]# chmod 777 /etc/my.cnf 
</code></pre></li>
<li><p>2.设置开机自启动服务控制脚本</p>
<pre><code>[root@localhost mysql]# cp ./support-files/mysql.server /etc/rc.d/init.d/mysqld     复制启动脚本到资源目录
[root@localhost mysql]# chmod +x /etc/rc.d/init.d/mysqld   增加mysqld服务控制脚本执行权限
[root@localhost mysql]# chkconfig --add mysqld    将mysqld服务加入到系统服务
[root@localhost mysql]# chkconfig --list mysqld    检查mysqld服务是否已经生效,命令输出类似下面的结果:
        mysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off 
        表明mysqld服务已经生效，在2、3、4、5运行级别随系统启动而自动启动，
        以后可以使用service命令控制mysql的启动和停止
        命令为:service mysqld start和service mysqld stop
</code></pre></li>
<li><p>3.启动mysqld</p>
<pre><code>[root@localhost mysql]# service mysqld start
</code></pre></li>
<li><p>4.将mysql的bin目录加入PATH环境变量，编辑 ~/.bash_profile文件</p>
<pre><code>[root@localhost mysql]# vim ~/.bash_profile
添加如下信息：
export PATH=$PATH:/usr/local/mysql/bin
之后：
[root@localhost mysql]# source ~/.bash_profile 
</code></pre></li>
<li><p>5.以root账户登录mysql,默认是没有密码的</p>
<pre><code>[root@localhost mysql]# mysql -uroot -p
</code></pre></li>
<li><p>6.设置root账户密码为root</p>
<pre><code>mysql&gt;use mysql;  
mysql&gt;update user set password=password(&apos;root&apos;) where user=&apos;root&apos; and host=&apos;localhost&apos;;  
mysql&gt;flush privileges;
</code></pre></li>
<li><p>7.设置远程主机登录，注意下面的your username 和 your password改成你需要设置的用户和密码</p>
<pre><code>mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;your username&apos;@&apos;%&apos; IDENTIFIED BY &apos;your password&apos; WITH  
GRANT OPTION; 
</code></pre></li>
<li><p>8.到此，在Centos 7上安装mysql5.6就完成了</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/16/Centos7安装MySQL5.6/" data-id="ck2ormw21000oi4wv7i31xq6l" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python之Map、Reduce、Filte" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/16/python之Map、Reduce、Filte/" class="article-date">
  <time datetime="2018-04-16T15:38:13.000Z" itemprop="datePublished">2018-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/16/python之Map、Reduce、Filte/">python之Map、Reduce、Filte</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>python之Map、Reduce、Filte</h5><p></p>
<hr>
<hr>
<p></p><h6>Map</h6><p></p>
<ul>
<li>简介<ul>
<li>map()函数接收两个及以上参数，一个是函数对象本身，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回</li>
<li>在Python2里，直接使用map就可以打印结果；但是在Python3里，map返回的结果是迭代器(iterator)，需要先转换为列表list</li>
</ul>
</li>
<li><p>计算列表中每个元素的的平方并输出</p>
<pre><code>def f(x):
    return x*x
list1=[1,2,3,4,5,6]
第一种方法：循环
list2=[]
for n in list1:
    list2.append(f(n))
print(list2)              #result:[1, 4, 9, 16, 25, 36]

第二种方法：map
list3=list(map(f,list1))
print(list3)              #result:[1, 4, 9, 16, 25, 36]
</code></pre></li>
<li><p>map()作为高阶函数，事实上它把运算规则抽象了，因此，我们不但可以计算简单的f(x)=x*x，还可以计算任意复杂的函数，比如，把这个list1所有数字转为字符串</p>
<pre><code>list4=list(map(str,list1))
print(list4)             #result:[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;]
</code></pre></li>
</ul>
<p></p><h6>Reduce</h6><p></p>
<ul>
<li>简介<ul>
<li>reduce把一个函数作用在一个序列[x1, x2, x3…]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算</li>
<li>reduce可以有initial参数</li>
</ul>
</li>
<li><p>示例</p>
<pre><code>from functools import reduce
#没有initial参数
sum1=reduce(lambda x,y:x+y,[1,2,3,4,5,6])    #（（（1+2）+3+）+4...）
print(sum1)             #result:21

#有initial参数
sum2=reduce(lambda x,y:x+y,[1,2,3,4,5,6],10)  #（（（（10+1）+2）+3）+4...）
print(sum2)             #result:31
</code></pre></li>
</ul>
<p></p><h6>Filte</h6><p></p>
<ul>
<li>简介<ul>
<li>filter函数对指定的序列进行过滤操作</li>
<li>filter()也接收一个函数和一个序列，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素</li>
</ul>
</li>
<li><p>示例</p>
<pre><code>list4=[1,2,3,4,5,6,7,8,9]
def is_odd(x):
    return x%2==1
result=list(filter(is_odd,list4))
print(result)               #result:[1, 3, 5, 7, 9]
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/16/python之Map、Reduce、Filte/" data-id="ck2ormwb2004ki4wvz2d1dtat" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python学习/">Python学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-安装python3-6、spark" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/16/安装python3-6、spark/" class="article-date">
  <time datetime="2018-04-16T11:13:35.000Z" itemprop="datePublished">2018-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/16/安装python3-6、spark/">安装python3.6、spark</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>安装python3.6、spark</h5><p></p>
<hr>
<hr>
<p></p><h6>安装python3.6,，并可以通过页面进行编程</h6><p></p>
<ul>
<li><p>1.普通用户（hadoop用户）将文件Anaconda3-5.0.1-Linux-x86_64.sh放在工作目录下</p>
</li>
<li><p>2.切换root用户安装bzip2：<code>yum -y install bzip2</code></p>
</li>
<li>3.hadoop用户执行命令：<code>bash Anaconda3-5.0.1-Linux-x86_64.sh</code>，该命令执行过程中有一步可以自动配置环境变量，注意查看并输入yes进行确认</li>
<li>4.刷新<code>source .bashrc</code>后，输入<code>python</code>后输出了python的最新版本信息</li>
<li>5.hadoop用户执行：<code>jupyter-notebook --ip python333</code>（该命令必须启动yarn，我的主机名叫python333）后，提示：<br><code>http://python333:8888/?token=b528b9bf6f867f2471ee75ac04be8b46d30c957e2c76cadf</code><br><br>在浏览器新开窗口中输入该url后即打开了<code>http://python333:8888/tree</code>页面</li>
<li>6.在打开的jupyter页面中，“New”一个新的notebook，开始编辑代码吧</li>
</ul>
<p></p><h6>安装spark</h6><p></p>
<ul>
<li>利用spark中的MapReduce比使用hadoop中的MapReduce运行速度快近100倍，但是spark不能建立集群文件，需要依赖hadoop</li>
<li><p>开始安装</p>
<ul>
<li>1.普通用户（hadoop用户）将文件spark-2.2.1-bin-hadoop2.7.tgz放在工作目录下，并解压：<code>tar -zvxf spark-2.2.1-bin-hadoop2.7.tgz -C opt/</code></li>
<li><p>2.还是在该工作目录下，<code>vi .bashrc</code>后追加如下变量：</p>
<pre><code>SPARK_HOME=/home/hadoop/opt/spark-2.2.1-bin-hadoop2.7
PATH=$PATH:$SPARK_HOME/bin
</code></pre></li>
<li><p>3.刷新<code>source .bashrc</code></p>
</li>
<li>4.现在可以在任意的目录下输入spark-shell,之后显示图标并进入scala命令行</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/16/安装python3-6、spark/" data-id="ck2ormwfi006gi4wv6ck2jxd2" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-使用hadoop里面的MapReduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/16/使用hadoop里面的MapReduce/" class="article-date">
  <time datetime="2018-04-16T10:59:19.000Z" itemprop="datePublished">2018-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/16/使用hadoop里面的MapReduce/">使用hadoop里面的MapReduce</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>使用hadoop里面的MapReduce</h5><p></p>
<hr>
<hr>
<p></p><h6>示例：使用hadoop里面的MapReduce统计data1中有哪些内容及字段出现频率</h6><p></p>
<ul>
<li>1.普通用户（hadoop）启动服务，<code>start-all.sh</code>后在浏览器中输入：<code>python333:50070</code>，就是主机名与端口号，建立与HDFS的关联</li>
<li>2.hadoop用户在工作目录下<code>touch data1.txt</code>，并新增任意内容</li>
<li><p>3.创建文件上传后的目录</p>
<pre><code>hadoop fs -mkdir -p /user/hadoop   #-p为创建多级目录，该目录为文件上传路径
注：如果创建过程出现错误，如‘Name node is in safe mode’,需要进入hadoop-2.9.0目录下，
执行命令：bin/hadoop dfsadmin -safemode leave
</code></pre></li>
<li><p>4.将本地文件data1.txt上传到hdfs上</p>
<pre><code>hadoop fs -put data1.txt 
</code></pre></li>
<li><p>5.统计字段个数</p>
<ul>
<li><p>5.1进入/home/hadoop/opt/hadoop-2.9.0/share/hadoop/mapreduce下，执行命令：</p>
<pre><code>hadoop jar hadoop-mapreduce-examples-2.9.0.jar wordcount /user/hadoop/ /user/output
#该命令后两个路径，其中前一个为需要解析统计的文件路径，后一个为文件统计后的存放路径
</code></pre></li>
<li><p>5.2可在浏览器打开的“Browse Directory”中输入<code>/user/output</code>查看统计结果，结果的文件名为part-r-00000</p>
</li>
<li><p>5.3hadoop用户可以回到工作目录下，输入一下命令将结果文件下载到工作目录下</p>
<pre><code>hadoop fs -get /user/output/part-r-00000
</code></pre><p>5.4<code>cat part-r-00000</code>可以查看结果文件内容</p>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/16/使用hadoop里面的MapReduce/" data-id="ck2ormwdt005oi4wvh7ytwyi5" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-虚拟机centos上部署hadoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/14/虚拟机centos上部署hadoop/" class="article-date">
  <time datetime="2018-04-14T10:10:13.000Z" itemprop="datePublished">2018-04-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/14/虚拟机centos上部署hadoop/">虚拟机centos上部署hadoop</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>虚拟机centos上部署hadoop</h5><p></p>
<hr>
<hr>
<p></p><h6>基本配置</h6><p></p>
<ul>
<li>以下1-5的操作均以root用户身份针对虚拟机中的Linux系统</li>
<li><p>1.root下分别配置静态IP、子网掩码、网关、域名解析</p>
<pre><code>IPADDR=192.168.220.128    （linux系统的IP）
NETMASK=255.255.255.0
GATEWAY=192.168.220.2    （linux系统的网关）
DNS1=202.106.0.20       （可以是这个）
</code></pre></li>
<li><p>2.root下vi /etc/sysconfig/network-scripts/ifcfg-ens33后追加如下内容：</p>
<pre><code>IPADDR=192.168.220.128
NETMASK=255.255.255.0
GATEWAY=192.168.220.2
DNS1=202.106.0.20

并修改BOOTPROTO=static
</code></pre></li>
<li><p>3.重启网络：systemctl restart network</p>
</li>
<li><p>4.root下修改主机名称<br>默认情况下的主机名:localhost，修改为python333</p>
<pre><code>vi /etc/hostname后只留存内容：python333
</code></pre><p>  修改主机映射</p>
<pre><code>vi /etc/hosts后追加内容：192.168.220.128 python333
</code></pre></li>
<li><p>5.重启linux系统，命令：reboot</p>
</li>
<li>6.修改Windows主机对虚拟机中linux系统的认知<br>在C:\Windows\System32\drivers\etc下的hosts中追加192.168.220.128 python333<br>即可以ssh python333远程登录linux</li>
</ul>
<p></p><h6>部署hadoop</h6><p></p>
<ul>
<li>以下步骤多以hadoop用户身份操作，即以普通用户操作</li>
<li><p>1.hadoop用户解压文件于/home/hadoop/opt/下,命令：tar -zvxf hadoop-xxx.gx -C opt</p>
</li>
<li><p>2.配置hadoop环境变量,hadoop用户在工作目录下vi .bashrc后追加如下内容：</p>
<pre><code>export HADOOP_HOME=/home/hadoop/opt/hadoop-2.9.0
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</code></pre></li>
<li><p>3.之后source .bashrc刷新，输入hadoop验证</p>
</li>
<li><p>4.配置hadoop配置文件，进入/home/hadoop/opt/hadoop-2.9.0/etc/hadoop下，添加如下信息</p>
<pre><code>4.1.core-site.xml:默认文件系统hdfs，HDFS浏览器请求地址
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://python333:9000&lt;/value&gt; 
    &lt;/property&gt;

4.2.hdfs-site,xml：修改 Hadoop 文件块的默认备份数3为1
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
        &lt;value&gt;file:///home/hadoop/opt/tmp/dfs/name&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
        &lt;value&gt;file:///home/hadoop/opt/tmp/dfs/data&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
        &lt;value&gt;python333:50070&lt;/value&gt;
    &lt;/property&gt;

4.3.mapred-site.xml：启用yarn的资源调度框架，
注：需要备份cp mapred-site.xml.template mapred-site.xml
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;

4.4.yarn-site.xml：配置yarn主机  
    &lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
        &lt;value&gt;python333&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;

4.5.slaves：配置dataname主机名称
    python333
</code></pre></li>
<li><p>5.root用户关闭防火墙</p>
<ul>
<li><p>5.1.选择永久关闭（临时关闭：setenforce 0）<br></p>
<pre><code>vi /etc/selinux/config后修改：SELINUX=disabled
</code></pre></li>
<li><p>5.2关闭防火墙（查看防火墙状态systemctl status firewalld）</p>
<pre><code>临时关闭 systemctl stop firewalld
永久关闭 systemctl disable firewalld
</code></pre></li>
<li><p>5.3重启电脑reboot</p>
</li>
</ul>
</li>
<li><p>6.退出管理员用户，以hadoop进入/home/hadoop/opt/下，</p>
<ul>
<li>6.1新建tmp目录：mkdir tmp</li>
<li>6.2执行hdfs文件系统格式化,输入：hdfs namenode -format</li>
</ul>
</li>
<li><p>7.配置密匙（公匙，私匙；可以在工作目录下）</p>
<ul>
<li>7.1执行ssh-keygen -t rsa，一路回车生成密匙</li>
<li>7.2想无密码登陆到哪台电脑：ssh-copy-id python333，之后输入yes确认</li>
</ul>
</li>
<li><p>8.8.验证（hadoop在工作目录下）</p>
<ul>
<li>8.1输入start-all.sh</li>
<li>8.2输入jps</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/14/虚拟机centos上部署hadoop/" data-id="ck2ormwkc0088i4wvpyrkwcyo" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大数据/">大数据</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-使用LinkExtractor提取链接" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/14/使用LinkExtractor提取链接/" class="article-date">
  <time datetime="2018-04-14T10:08:49.000Z" itemprop="datePublished">2018-04-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/14/使用LinkExtractor提取链接/">使用LinkExtractor提取链接</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>使用LinkExtractor提取链接</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li>在爬取一个网站时，想要爬取的数据通常分布在多个页面中，每个页面包含一部分数据以及到其他页面的链接，提取链接有使用Selector和使用LinkExtractor两种方法</li>
<li><p>使用Selector，因为链接也是页面中的数据，所以可以使用与提取数据相同的方法进行提取，在提取少量（几个）链接或提取规则比较简单时，使用Selector就足够了</p>
<pre><code>next_url = response.css(&apos;ul.pager li.next a::attr(href)&apos;).extract_first()      
if next_url:  # 如果找到下一页的url，得到绝对路径，构造新的Request 对象          
    next_url = response.urljoin(next_url) 
     yield scrapy.Request(next_url, callback=self.parse)
</code></pre></li>
<li><p>Scrapy提供了一个专门用于提取链接的类LinkExtractor，在提取大量链接或提取规则比较复杂时，使用LinkExtractor更加方便</p>
</li>
</ul>
<p></p><h6>详解</h6><p></p>
<ul>
<li><p>首先制造一个实验环境，创建两个包含多个链接的HTML页面，使用这两个HTML文本构造两个Response对象</p>
<pre><code>from scrapy.http import HtmlResponse
from scrapy.linkextractors import LinkExtractor
html1=&apos;&apos;&apos;
    &lt;html&gt;
        &lt;body&gt;      
            &lt;div id=&quot;top&quot;&gt;         
                &lt;p&gt;下面是一些站内链接&lt;/p&gt;         
                &lt;a class=&quot;internal&quot; href=&quot;/intro/install.html&quot;&gt;Installation guide&lt;/a&gt;
                &lt;a class=&quot;internal&quot; href=&quot;/intro/tutorial.html&quot;&gt;Tutorial&lt;/a&gt;         
                &lt;a class=&quot;internal&quot; href=&quot;../examples.html&quot;&gt;Examples&lt;/a&gt;      
            &lt;/div&gt;      
            &lt;div id=&quot;bottom&quot;&gt;         
                &lt;p&gt;下面是一些站外链接&lt;/p&gt;         
                &lt;a href=&quot;http://stackoverflow.com/tags/scrapy/info&quot;&gt;StackOverflow&lt;/a&gt;         
                &lt;a href=&quot;https://github.com/scrapy/scrapy&quot;&gt;Fork on Github&lt;/a&gt;      
            &lt;/div&gt;
        &lt;/body&gt;
    &lt;/html&gt;
&apos;&apos;&apos;
html2=&apos;&apos;&apos;
    &lt;html&gt;   
        &lt;head&gt;       
            &lt;script type=&apos;text/javascript&apos; src=&apos;/js/app1.js&apos;/&gt;       
            &lt;script type=&apos;text/javascript&apos; src=&apos;/js/app2.js&apos;/&gt;   
        &lt;/head&gt;   
        &lt;body&gt;       
            &lt;a href=&quot;/home.html&quot;&gt;主页&lt;/a&gt;       
            &lt;a href=&quot;javascript:goToPage(&apos;/doc.html&apos;); return false&quot;&gt;文档&lt;/a&gt;       
            &lt;a href=&quot;javascript:goToPage(&apos;/example.html&apos;); return false&quot;&gt;案例&lt;/a&gt;   
        &lt;/body&gt;
    &lt;/html&gt;
&apos;&apos;&apos;
response1=HtmlResponse(url=&apos;http://example1.com&apos;,body=html1,encoding=&apos;utf-8&apos;)
response2=HtmlResponse(url=&apos;http://example2.com&apos;,body=html2,encoding=&apos;utf-8&apos;)
</code></pre></li>
<li><p>特例情况，LinkExtractor构造器的所有参数都有默认值，如果构造对象时不传递任何参数（使用默认值），就提取页面中所有链接</p>
<pre><code>le=LinkExtractor()
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中所有链接，5个
</code></pre></li>
<li><p>allow<br>接收一个正则表达式或一个正则表达式列表，提取绝对url与正则表达式匹配的链接</p>
<pre><code>示例　提取页面example1.html中路径以/intro开始的链接：
pattern=r&apos;/intro/.+.html$&apos;
le=LinkExtractor(allow=pattern)
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中2个以/intro开始的链接
</code></pre></li>
<li><p>deny<br>接收一个正则表达式或一个正则表达式列表，与allow相反，排除绝对url与正则表达式匹配的链接</p>
<pre><code>示例　提取页面example1.html中所有站外链接（即排除站内链接）
pattern=r&apos;.*.html$&apos;
le=LinkExtractor(deny=pattern)
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中2个站外链接
</code></pre></li>
<li><p>allow_domains<br>接收一个域名或一个域名列表，提取到指定域的链接</p>
<pre><code>示例　提取页面example1.html中所有到github.com和stackoverflow.com这两个域的链接
domains=[&apos;stackoverflow.com&apos;,&apos;github.com&apos;]
le=LinkExtractor(allow_domains=domains)
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中2个站外链接
</code></pre></li>
<li><p>deny_domains<br>接收一个域名或一个域名列表，与allow_domains相反,排除到指定域的链接</p>
<pre><code>示例　提取页面example1.html中除了到github.com域以外的链接
domains=[&apos;github.com&apos;]
le=LinkExtractor(deny_domains=domains)
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中除了github外的4个链接
</code></pre></li>
<li><p>restrict_xpaths<br>接收一个XPath表达式或一个XPath表达式列表，提取XPath表达式选中区域下的链接</p>
<pre><code>示例　提取页面example1.html中&lt;div id=&quot;top&quot;&gt;元素下的链接：
le=LinkExtractor(restrict_xpaths=&apos;//div[@id=&quot;top&quot;]&apos;)
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中3个站内链接
</code></pre></li>
<li><p>restrict_css<br>接收一个CSS选择器或一个CSS选择器列表，提取CSS选择器选中区域下的链接</p>
<pre><code>示例　提取页面example1.html中&lt;div id=&quot;bottom&quot;&gt;元素下的链接
le=LinkExtractor(restrict_css=&apos;#bottom&apos;)
links=le.extract_links(response1)
print([link.url for link in links])   #打印出html1中2个站外链接
</code></pre></li>
<li><p>tags<br>接收一个标签（字符串）或一个标签列表，提取指定标签内的链接，默认为[‘a’, ‘area’]<br>attrs<br>接收一个属性（字符串）或一个属性列表，提取指定属性内的链接，默认为[‘href’]</p>
<pre><code>示例　提取页面example2.html中引用JavaScript文件的链接
le=LinkExtractor(tags=&apos;script&apos;,attrs=&apos;src&apos;)
links=le.extract_links(response2)
print([link.url for link in links])   #打印出html2中2个script链接
</code></pre></li>
<li><p>process_value<br>接收一个形如func(value)的回调函数。如果传递了该参数，LinkExtractor将调用该回调函数对提取的每一个链接（如a的href）进行处理，回调函数正常情况下应返回一个字符串（处理结果），想要抛弃所处理的链接时，返回None</p>
<pre><code>示例　在页面example2.html中，某些a的href属性是一段JavaScript代码，代码中包含了链接页面的实际url地址，此时应对链接进行处理，提取页面example2.html中所有实际链接
import re
def process(value):
    m=re.search(&quot;javascript:goToPage\(&apos;(.*?)&apos;&quot;,value)
    if m:
        value=m.group(1)
    return value
le=LinkExtractor(process_value=process)
links=le.extract_links(response2)
print([link.url for link in links])   #打印出html2中a标签中的3个链接
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/14/使用LinkExtractor提取链接/" data-id="ck2ormwcz005ei4wvrqu6lcpj" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-linux文件的打包、压缩" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/12/linux文件的打包、压缩/" class="article-date">
  <time datetime="2018-04-12T13:37:47.000Z" itemprop="datePublished">2018-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/12/linux文件的打包、压缩/">linux文件的打包、压缩</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>linux文件的打包、压缩</h5><p></p>
<hr>
<hr>
<p></p><h6>Linux常见的压缩文件扩展名</h6><p></p>
<ul>
<li>.gz　　    gzip压缩的文件</li>
<li>.bz2　　    bzip2压缩的文件</li>
<li>.tar　　    tar程序打包的文件</li>
<li>.tar.gz　　tar程序打包，gzip压缩的文件</li>
<li>.tar.bz2　　tar程序打包，bzip2压缩的文件</li>
</ul>
<p></p><h6>gzip命令</h6><p></p>
<ul>
<li><p>使用最广的解压缩命令，可以解开zip，gzip压缩过的文件</p>
<pre><code>gzip  [-cdtv#]  [文件名] &gt; [压缩后文件名]
-c　　：将压缩的数据输出到屏幕上，通过数据流重定向来处理
-d　　：解压缩参数
-t　　：用来检验压缩文件的一致性，看看文件是否有错误
-v　　：输出文件压缩比等信息
-#　　：压缩等级０－９，１－９压缩比增大速度变慢，默认为６
</code></pre></li>
<li><p>示例</p>
<pre><code>gzip -cv data1.txt &gt; data1.gz  (压缩)
gzip -d data1.gz               (解压)
</code></pre></li>
</ul>
<p></p><h6>bzip2命令</h6><p></p>
<ul>
<li><p>比gzip压缩更好的压缩命令</p>
<pre><code>bzip2   [-cdkzv#]　文件名
-c　　：将压缩过程中产生的数据输出到屏幕上
-d　　：解压缩的参数
-k　　：保留原文件
-z　　：压缩的参数
-v　　:可以显示出原文件/压缩文件的压缩比
-#　　：同gzip
</code></pre></li>
<li><p>示例</p>
<pre><code>如果使用的linux的版本是centos7的话需要yum install bzip2
bzip2 -kzv data.txt      (压缩)
bzip2 -d data.txt.bz2    (解压)
</code></pre></li>
</ul>
<p></p><h6>tar命令</h6><p></p>
<ul>
<li><p>哈哈，这个比较多，也更加实用</p>
<pre><code>tar　[-j|z]  [-cv]  [-f  新建的文件名]  filename 　　//打包与压缩
tar  [-j|z]  [-tv]  [-f  新建的文件名]  filename　　//查看文件名
tar  [-j|z]　[-xv]  [-f  新建的文件名]　[-C 目录]　　//解压缩

参数：
-c　　：新建打包文件
-t　　：查看打包文件的内容中有那些文件名
-x　　：解打包或解压缩
以上3条不可出现在同一串命令中
-j　　：通过bzip2的支持来压缩/解压缩
-z　　：通过gzip的支持来压缩/解压缩
-v　　：压缩/解压缩过程中显示正在处理的文件名
-f　　：后面接要处理的文件名
-C　　：特定目录解压
-p　　：保留备份数据的　原本权限与属性
</code></pre></li>
<li><p>重点说一下常用到的命令</p>
<pre><code>gzip压缩：tar  -zcvf  filename.tar.gz  要压缩的文件目录或名称
gzip解压：tar  -zxvf  filename.tar.gz　-C　解压的目录
gzip查看：tar  -zcvf  filename.tar.gz　
bzip2压缩：tar  -jcvf  filename.tar.bz2  要压缩的文件目录或名称
bzip2解压：tar  -jxvf  filename.tar.bz2　-C　解压的目录
bzip2查看：tar  -jcvf  filename.tar.bz2　
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/12/linux文件的打包、压缩/" data-id="ck2ormwag004ci4wvmciusvlm" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux学习/">Linux学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-jdk部署" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/11/jdk部署/" class="article-date">
  <time datetime="2018-04-11T11:48:13.000Z" itemprop="datePublished">2018-04-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/11/jdk部署/">jdk部署</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>jdk部署</h5><p></p>
<hr>
<hr>
<p></p><h6>echo遍历、输出、打印</h6><p></p>
<ul>
<li><p>输出</p>
<pre><code>name=&apos;hadoop&apos;(定义变量)
echo $name（输出变量值）
</code></pre></li>
<li><p>重定向、覆盖</p>
<pre><code>touch data1.txt(里面有内容)
echo ookk &gt; data1.txt(此时文件中内容被“ookk”覆盖)
</code></pre></li>
<li><p>追加</p>
<pre><code>touch data2.txt(里面有内容)
echo ookk &gt;&gt;data2.txt(此时文件末尾追加字段“ookk”)
</code></pre></li>
</ul>
<p></p><h6>jdk部署</h6> <p></p>
<ul>
<li><p>按照课堂讲解内容整理</p>
<pre><code>1.root下新建用户useradd hadoop,并给用户hadoop设置密码passwd hadoop
2.切换到hadoop下，su hadoop之后cd /home/hadoop下，即工作区；或者使用hadoop登录，默认进入工作区
3.新建目录mkdir opt
4.将文件jdk-8u152-linux-x64.tar.gz解压到opt目录下，命令为：
tar -zvxf jdk-8u152-linux-x64.tar.gz -C /home/hadoop/opt
5.配置jdk（两种方式）
    第一种：在hadoop的主目录下，vi .bashrc后添加：
        export JAVA_HOME=/home/hadoop/opt/jdk1.8.0_152
        export PATH=$JAVA_HOME/bin:$PATH
        source ~/.bashrc刷新
    第二种：以追加的形式，hadoop用户
        cd /opt/jdk1.8.0_152下，
        设置变量java_home=`pwd`，
        echo export JAVA_HOME=$java_home &gt;&gt; ~/.bashrc
        echo export PATH=\$JAVA_HOME\/bin:\$PATH &gt;&gt; ~/.bashrc
        追加之后刷新source ~/.bashrc
6.输入java -version查看是否配置成功
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/11/jdk部署/" data-id="ck2ormw96003si4wvbpj1vie2" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux学习/">Linux学习</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/5/">&laquo; zurück</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/7/">weiter &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django学习/">Django学习</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JAVA项目部署/">JAVA项目部署</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux学习/">Linux学习</a><span class="tag-list-count">16</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL学习/">MySQL学习</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nginx学习/">Nginx学习</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NoSQL学习/">NoSQL学习</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python学习/">Python学习</a><span class="tag-list-count">27</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web前端/">Web前端</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/安全测试/">安全测试</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/性能测试/">性能测试</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫学习/">爬虫学习</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网络基础知识/">网络基础知识</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/自动化测试/">自动化测试</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/软件工具/">软件工具</a><span class="tag-list-count">6</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Django学习/" style="font-size: 11.11px;">Django学习</a> <a href="/tags/JAVA项目部署/" style="font-size: 16.67px;">JAVA项目部署</a> <a href="/tags/Linux学习/" style="font-size: 17.78px;">Linux学习</a> <a href="/tags/MySQL学习/" style="font-size: 18.89px;">MySQL学习</a> <a href="/tags/Nginx学习/" style="font-size: 10px;">Nginx学习</a> <a href="/tags/NoSQL学习/" style="font-size: 16.67px;">NoSQL学习</a> <a href="/tags/Python学习/" style="font-size: 20px;">Python学习</a> <a href="/tags/Web前端/" style="font-size: 14.44px;">Web前端</a> <a href="/tags/大数据/" style="font-size: 12.22px;">大数据</a> <a href="/tags/安全测试/" style="font-size: 10px;">安全测试</a> <a href="/tags/性能测试/" style="font-size: 14.44px;">性能测试</a> <a href="/tags/爬虫学习/" style="font-size: 15.56px;">爬虫学习</a> <a href="/tags/网络基础知识/" style="font-size: 10px;">网络基础知识</a> <a href="/tags/自动化测试/" style="font-size: 13.33px;">自动化测试</a> <a href="/tags/软件工具/" style="font-size: 12.22px;">软件工具</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">18</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">46</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">8</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/11/07/redis中hash基本操作（删改）/">redis中hash基本操作（删、改）</a>
          </li>
        
          <li>
            <a href="/2019/08/06/磁盘挂载/">磁盘挂载</a>
          </li>
        
          <li>
            <a href="/2019/08/06/配置MongoDB访问账号-密码/">配置MongoDB访问账号&amp;密码</a>
          </li>
        
          <li>
            <a href="/2019/05/28/读书乱纪01/">笔记01</a>
          </li>
        
          <li>
            <a href="/2019/05/11/mysql日志文件/">mysql日志文件</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">About</h3>
    <div class="widget">
       Email:<a>liuyongqian51@163.com</a><br />
          QQ:<a>272501447</a><br />
	  Github:<a></a>
    </div>
  </div>

  
  

</aside>


        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 刘永前<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>

</footer>


    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>