<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>LiuYongQian</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="永远相信美好的事情将要发生！">
<meta property="og:type" content="website">
<meta property="og:title" content="LiuYongQian">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="LiuYongQian">
<meta property="og:description" content="永远相信美好的事情将要发生！">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LiuYongQian">
<meta name="twitter:description" content="永远相信美好的事情将要发生！">
  
    <link rel="alternate" href="/atom.xml" title="LiuYongQian" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">LiuYongQian</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">永远相信美好的事情将要发生！</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-pyspider框架" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/24/pyspider框架/" class="article-date">
  <time datetime="2018-03-24T10:41:19.000Z" itemprop="datePublished">2018-03-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/24/pyspider框架/">pyspider框架</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>pyspider框架</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li>pyspider 是一个用python实现的功能强大的网络爬虫系统，能在浏览器界面上进行脚本的编写，功能的调度和爬取结果的实时查看，后端使用常用的数据库进行爬取结果的存储，还能定时设置任务与任务优先级等</li>
<li>PhantomJS 是一个基于 WebKit 的服务器端 JavaScript API。它全面支持web而不需浏览器支持，其快速、原生支持各种Web标准：DOM 处理、CSS 选择器、JSON、Canvas 和 SVG。 PhantomJS 可以用于页面自动化、网络监测、网页截屏以及无界面测试等</li>
<li>安装完pyspider、phantomJS后，在命令行输入<code>pyspider all</code>启动pyspider所有服务组件…然后浏览器访问 <a href="http://localhost:5000，如果正常出现" target="_blank" rel="noopener">http://localhost:5000，如果正常出现</a> PySpider 的页面，那证明一切OK</li>
<li>pyspider的架构主要分为 scheduler（调度器）, fetcher（抓取器）, processor（脚本执行）,以及一个监控组件，当启动服务组件的时候，这些程序均开始运行了…<ul>
<li>1.各个组件间使用消息队列连接，除了scheduler是单点的，fetcher 和 processor 都是可以多实例分布式部署的。 scheduler 负责整体的调度控制</li>
<li>2.任务由 scheduler 发起调度，fetcher 抓取网页内容， processor 执行预先编写的python脚本，输出结果或产生新的提链任务（发往 scheduler），形成闭环</li>
<li>3.每个脚本可以灵活使用各种python库对页面进行解析，使用框架API控制下一步抓取动作，通过设置回调控制解析动作</li>
</ul>
</li>
</ul>
<p></p><h6>页面内容解析</h6><p></p>
<ul>
<li>拿www.reeoo.com这个网页，爬取上面的数据</li>
<li>创建新项目页面：<ul>
<li>Project Name：任务的名字</li>
<li>Start URL(s)：爬取任务开始的地址</li>
</ul>
</li>
<li><p>创建后的页面</p>
<pre><code>from pyspider.libs.base_handler import *
class Handler(BaseHandler):
    crawl_config = {
    }

    @every(minutes=24 * 60)
    #@every(minutes=24 * 60) 通知 scheduler（框架的模块） 每天运行一次
    def on_start(self):
    #on_start(self) 程序的入口，当点击左侧绿色区域右上角的 run 按钮时首先会调用这个函数
        self.crawl(&apos;https://reeoo.com/&apos;, callback=self.index_page)
    #self.crawl(url, callback) pyspider库主要的API，用于创建一个爬取任务，url 为目标地址，这里为我们刚刚创建任务指定的起始地址，callback 为抓取到数据后的回调函数

    @config(age=10 * 24 * 60 * 60)
    #@config(age=10 * 24 * 60 * 60) 设置任务的有效期限，在这个期限内目标爬取的网页被认为不会进行修改
    def index_page(self, response):
    #index_page(self, response) 参数为 Response 对象，response.doc 为 pyquery 对象，主要用来方便地抓取返回的html文档中对应标签的数据
        for each in response.doc(&apos;a[href^=&quot;http&quot;]&apos;).items():
            self.crawl(each.attr.href, callback=self.detail_page)

    @config(priority=2)
    #@config(priority=2) 设定任务优先级
    def detail_page(self, response):
    #detail_page(self, response) 返回一个 dict 对象作为结果，结果会自动保存到默认的 resultdb 中，也可以通过重载方法来将结果数据存储到指定的数据库
        return {
            &quot;url&quot;: response.url,
            &quot;title&quot;: response.doc(&apos;title&apos;).text(),
        }
</code></pre></li>
<li><p>运行代码后时出现 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误时：<br><br>使用 self.crawl(url, callback=self.index_page, validate_cert=False)</p>
</li>
<li>运行<br>1.点击左边绿色区域右上角的 run 按钮，运行之后页面下册的 follows 按钮出现红色角标<br>2.选中 follows 按钮，看到 index_page 行，点击行右侧的运行按钮<br>3.运行完成后显示 www.reeoo.com 页面上所有的url<br>4.此时任意选择一个结果运行，此时调用的是 detail_page 方法，返回结果为json格式的数据，这里我们保存的是网页的 title 和 url，见左侧黑色的区域<br>5.回到主页面，此时看到任务列表显示刚刚创建的任务，设置 status 为 running，然后点击 Run 按钮执行<br>6.执行过程中可以看到整个过程的打印输出<br>7.执行完成后，点击 Results 按钮，进入到爬取结果的页面<br>8.右上方的按钮选择将结果数据保存成对应的格式，例如：JSON格式的数据</li>
</ul>
<p></p><h6>查找文件路径</h6><p></p>
<ul>
<li>由于配置了windows的环境变量，所以习惯性的打开CMD后就直接敲命令行执行 pyspider语句，但找不到pyspider存储的数据在哪里</li>
<li>后来才发现，pyspider命令行执行的时候，数据库data文件会自动在当前目录生成，即在<code>C:\Users\DELL&gt;</code>下，<code>C:\Users\DELL\data</code></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/24/pyspider框架/" data-id="cjm7bd0po003ds8wvpqm28e7s" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-selenium模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/23/selenium模块/" class="article-date">
  <time datetime="2018-03-23T15:15:47.000Z" itemprop="datePublished">2018-03-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/23/selenium模块/">selenium模块</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>selenium模块</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li>selenium 是一套完整的web应用程序测试系统，Selenium的核心Selenium Core基于JsUnit，完全由JavaScript编写，因此可以用于任何支持JavaScript的浏览器上，爬虫中主要用来解决JavaScript渲染问题</li>
<li>Selenium.Webdriver支持的浏览器中，比较重要的PhantomJS,PhantomJS是一个基于WebKit的服务端JavaScript API,支持Web而不需要浏览器支持，其快速、原生支持各种Web标准：Dom处理，CSS选择器，JSON等等。PhantomJS可用于页面自动化、网络监测、网页截屏，以及无界面测试</li>
</ul>
<p></p><h6>查找元素</h6><p></p>
<ul>
<li><p>单个元素和多个元素的查找，用法基本一致</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.common.by import By
from time import sleep

browser=webdriver.Chrome()
browser.get(&apos;http://www.baidu.com&apos;)  #声明浏览器对象
#print(browser.page_source)       #打印百度首页的源代码
input=browser.find_element(By.ID,&apos;kw&apos;)
input.clear()
input.send_keys(&apos;selenium&apos;)
button=browser.find_element(By.CSS_SELECTOR,&apos;#su&apos;)
button.click()
sleep(3)
browser.close()
</code></pre></li>
<li><p>这里列举一下常用的查找元素方法：</p>
<pre><code>find_element_by_name
find_element_by_id
find_element_by_xpath
find_element_by_link_text
find_element_by_partial_link_text
find_element_by_tag_name
find_element_by_class_name
find_element_by_css_selector
</code></pre></li>
</ul>
<p></p><h6>执行JavaScript</h6><p></p>
<ul>
<li><p>这是一个非常有用的方法，这里就可以直接调用js方法来实现一些操作，下面的例子是通过登录知乎然后通过js翻到页面底部，并弹框提示</p>
<pre><code>from selenium import webdriver

browser=webdriver.Chrome()
browser.get(&apos;http://www.zhihu.com/explore&apos;)  #声明浏览器对象
browser.execute_script(&apos;window.scrollTo(0,document.body.scrollHeight)&apos;)
browser.execute_script(&apos;alert(&quot;Hello World!&quot;)&apos;)
</code></pre></li>
</ul>
<p></p><h6>01</h6><p></p>
<ul>
<li><p>获取属性值、文本值</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.common.by import By

browser=webdriver.Chrome()
browser.get(&apos;https://www.zhihu.com/explore&apos;)
logo=browser.find_element(By.ID,&apos;zh-top-link-logo&apos;)
print(logo.get_attribute(&apos;class&apos;))    #获取元素的属性值
print(logo.text)                      #获取文本值
print(logo.id)
print(logo.location)                  #获取位置
print(logo.tag_name)                  #获取标签名
print(logo.size)
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/23/selenium模块/" data-id="cjm7bd0qk003zs8wvrwwqvedh" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-pyquery模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/23/pyquery模块/" class="article-date">
  <time datetime="2018-03-23T15:15:27.000Z" itemprop="datePublished">2018-03-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/23/pyquery模块/">pyquery模块</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>pyquery模块</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li>PyQuery库也是一个非常强大又灵活的网页解析库，是 Python 仿照 jQuery 的严格实现</li>
<li><p>初始化<br>初始化的时候一般有三种传入方式：传入字符串，传入url,传入文件</p>
<pre><code>from pyquery import PyQuery as pq
html=&apos;&apos;&apos;
    &lt;div id=&quot;container&quot;&gt;
        &lt;ul class=&quot;list&quot;&gt;
             &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt;
             &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;
             &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
             &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;
             &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;
             &lt;p&gt;This is a paragraph.&lt;/p&gt;        
        &lt;/ul&gt;
    &lt;/div&gt;
&apos;&apos;&apos;
###字符串初始化
doc=pq(html)      #doc其实就是一个pyquery对象
print(doc(&apos;li&apos;))  #doc(标签名)就可以获取所有的该标签的内容
print(doc(&apos;.item-0&apos;))  #doc(&apos;.class_name&apos;)，doc(&apos;#id_name&apos;)
###URL初始化
url=&apos;http://www.baidu.com&apos;
doc=pq(url,encoding=&apos;utf-8&apos;)
print(doc(&apos;head&apos;))
###文件初始化
在pq()这里可以传入url参数也可以传入文件参数，当然这里的文件通常是一个html文件，例如：pq(filename=&apos;index.html&apos;)
</code></pre></li>
</ul>
<p></p><h6>查找元素</h6><p></p>
<ul>
<li><p>子元素（children,find）</p>
<pre><code>doc=pq(html)
items=doc(&apos;.list&apos;)
lis=items.find(&apos;li&apos;)   #lis=items.children()可以实现同样的效果
print(lis)
</code></pre></li>
<li><p>父元素（parent,parents）</p>
<pre><code>doc=pq(html)
items=doc(&apos;.list&apos;)
cont=items.parent() #可以找到父元素的所有内容
#cont=items.parents()返回两部分内容，一个是父节点信息，一个是父节点的父节点的信息即祖先节点的信息
print(cont)
</code></pre></li>
<li><p>兄弟元素（siblings）</p>
<pre><code>doc=pq(html)
li=doc(&apos;.list .item-0.active&apos;)
print(li.siblings())
</code></pre></li>
<li><p>遍历</p>
<pre><code>doc=pq(html)
lis=doc(&apos;li&apos;).items()
print(type(lis))     #&lt;class &apos;generator&apos;&gt;
for li in lis:
    print(li)        #通过for循环得到的每个元素依然是一个pyquery对象
</code></pre></li>
</ul>
<p></p><h6>获取信息</h6><p></p>
<ul>
<li><p>获取属性值–pyquery对象.attr(属性名)</p>
<pre><code>doc=pq(html)
a=doc(&apos;.list .item-1 a&apos;)
print(a.attr(&apos;href&apos;))
</code></pre></li>
<li><p>获取文本–pyquery对象.text()</p>
<pre><code>doc=pq(html)
a=doc(&apos;.list .item-0.active a&apos;)
print(a.text())
</code></pre></li>
<li><p>获取html–puquery对象.html()获取当前标签所包含的html信息</p>
<pre><code>doc=pq(html)
a=doc(&apos;.list .item-0.active a&apos;)
print(a.html())
</code></pre></li>
</ul>
<p></p><h6>DOM操作</h6><p></p>
<ul>
<li><p>addClass、removeClass添加和删除属性值</p>
<pre><code>doc=pq(html)
ul=doc(&apos;.list&apos;)
ul.addClass(&apos;2018&apos;)
ul.removeClass(&apos;2018&apos;)
print(ul)
</code></pre></li>
<li><p>attr,css给标签添加和修改属性</p>
<pre><code>doc=pq(html)
ul=doc(&apos;.list&apos;)
ul.attr(&apos;id&apos;,&apos;201803&apos;)
ul.css(&apos;font-size&apos;,&apos;15px&apos;)
print(ul)         #&lt;ul class=&quot;list&quot; id=&quot;201803&quot; style=&quot;font-size: 15px&quot;&gt;
</code></pre></li>
<li><p>remove将无用的或者干扰的标签直接删除</p>
<pre><code>doc=pq(html)
ul=doc(&apos;.list&apos;)
ul.find(&apos;p&apos;).remove()
print(ul.text())
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/23/pyquery模块/" data-id="cjm7bd0po003as8wvrxw8drmg" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-beautifulsoup模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/23/beautifulsoup模块/" class="article-date">
  <time datetime="2018-03-23T10:31:34.000Z" itemprop="datePublished">2018-03-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/23/beautifulsoup模块/">beautifulsoup模块</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>beautifulsoup模块</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li>BeautifulSoup是Python的一个库，最主要的功能就是从网页爬取我们需要的数据。BeautifulSoup将html解析为对象进行处理，全部页面转变为字典或者数组，相对于正则表达式的方式，可以大大简化处理过程</li>
<li>Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用 Python默认的解析器，lxml 解析器更加强大，速度更快，推荐安装</li>
<li><p>以下是这两个库的比较：</p>
<pre><code>1.Python标准库    BeautifulSoup(html,’html.parser’)    
Python内置标准库；执行速度快     容错能力较差
2.lxml HTML解析库    BeautifulSoup(html,’lxml’)    
速度快；容错能力强    需要安装，需要C语言库
</code></pre></li>
</ul>
<p></p><h6>标签选择器</h6><p></p>
<ul>
<li><p>soup.标签名，可以获得这个标签的内容；通过这种方式获取标签，如果文档中有多个这样的标签，返回的结果是第一个标签的内容</p>
<pre><code>from bs4 import BeautifulSoup
html=&apos;&apos;&apos;
    &lt;html&gt;
        &lt;head&gt;
            &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;
        &lt;/head&gt;
        &lt;body&gt;
            &lt;p class=&quot;story&quot;&gt;
                Once upon a time there were three little sisters; and their names were
                &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;
                    &lt;span&gt;Elsie&lt;/span&gt;
                &lt;/a&gt;
                &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;
                and
                &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;
                and they lived at the bottom of a well.
            &lt;/p&gt;
            &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;
&apos;&apos;&apos;
soup=BeautifulSoup(html,&apos;lxml&apos;)    #创建对象
print(soup.head.title.string)
</code></pre></li>
<li><p>基本用法</p>
<pre><code>soup.prettify()，自动补全缺失的标签
soup.标签名，可以获得这个标签的内容
soup.标签名.name，可以获得该标签的名称
soup.标签名[&apos;name&apos;]，可以获得该标签的name属性值
soup.标签名.string，可以获取第一个该标签的内容
soup.标签名.内标签名.string，嵌套方式
soup.标签名.contents，将该标签下的所有子标签存入到一个列表中
soup.标签名.parent，可以获取父节点所有信息
soup.标签名.next_sibling 获取下一个兄弟标签
soup.标签名.next_siblings 获取后面的兄弟节点
souo.标签名.previous_sinbling 获取上一个兄弟标签
soup.标签名.previous_siblings 获取前面的兄弟节点
</code></pre></li>
</ul>
<p></p><h6>标准选择器</h6><p></p>
<ul>
<li>soup.find_all(name,attrs,text)可根据标签名、属性、内容查找文档</li>
<li><p>soup.find(name,attrs,text)返回匹配结果的第一个元素</p>
<pre><code>from bs4 import BeautifulSoup
html=&apos;&apos;&apos;
    &lt;div class=&quot;panel&quot;&gt;
        &lt;div class=&quot;panel-heading&quot;&gt;
            &lt;h4&gt;Hello&lt;/h4&gt;
        &lt;/div&gt;
        &lt;div class=&quot;panel-body&quot;&gt;
            &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;
                &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;
                &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;
                &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;
            &lt;/ul&gt;
            &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;
                &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;
                &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;
            &lt;/ul&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&apos;&apos;&apos;
soup=BeautifulSoup(html,&apos;lxml&apos;)
for li in soup.find_all(&apos;ul&apos;):    #以针对结果再次find_all,从而获取所有的li标签信息
    print(li.find_all(&apos;li&apos;))
print(soup.find_all(attrs={&apos;class_&apos;:&apos;element&apos;}))
#attrs可以传入字典的方式来查找标签，但是这里有个特殊的就是class,因为class在python中是特殊的字段，所以如果想要查找class相关的可以更改attrs={&apos;class_&apos;:&apos;element&apos;}
</code></pre></li>
<li><p>基本用法：</p>
<pre><code>soup.find_all(&apos;标签名&apos;)  结果返回的是一个列表的方式
soup.finf_all(attrs={&apos;class_&apos;:&apos;element&apos;})
soup.find_all(text=&apos;Foo&apos;) 返回的是查到的所有的text=&apos;Foo&apos;的文本
</code></pre></li>
</ul>
<p></p><h6>CSS选择器</h6><p></p>
<ul>
<li>通过select()直接传入CSS选择器就可以完成选择</li>
<li>通过get_text()可以获取文本内容</li>
<li><p>通过 标签名[属性名] 或者 标签名.attrs[属性名]获取属性</p>
<pre><code>from bs4 import BeautifulSoup
html=&apos;&apos;&apos;
    &lt;div class=&quot;panel&quot;&gt;
        &lt;div class=&quot;panel-heading&quot;&gt;
            &lt;h4&gt;Hello&lt;/h4&gt;
        &lt;/div&gt;
        &lt;div class=&quot;panel-body&quot;&gt;
            &lt;ul class=&quot;list&quot; id=&quot;list-1&quot;&gt;
                &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;
                &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;
                &lt;li class=&quot;element&quot;&gt;Jay&lt;/li&gt;
            &lt;/ul&gt;
            &lt;ul class=&quot;list list-small&quot; id=&quot;list-2&quot;&gt;
                &lt;li class=&quot;element&quot;&gt;Foo&lt;/li&gt;
                &lt;li class=&quot;element&quot;&gt;Bar&lt;/li&gt;
            &lt;/ul&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&apos;&apos;&apos;
soup=BeautifulSoup(html,&apos;lxml&apos;)
print(soup.select(&apos;#list-2 .element&apos;))
print(soup.ul[&apos;id&apos;])
for li in soup.select(&apos;li&apos;):
    print(li.get_text())     #获得文本内容
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/23/beautifulsoup模块/" data-id="cjm7bd0ot002js8wvll0ao5c5" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-mongodb排序" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/22/mongodb排序/" class="article-date">
  <time datetime="2018-03-22T10:47:37.000Z" itemprop="datePublished">2018-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/22/mongodb排序/">mongodb排序</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>mongodb排序</h5><p></p>
<hr>
<hr>
<p></p><h6>排序<h6><p></p>
<ul>
<li>sort()方法对数据进行排序，sort()方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而-1是用于降序排列</li>
<li><p>语法：db.COLLECTION_NAME.find().sort({KEY:1})</p>
<pre><code># 插入数据库的数据如下：
{
    title: &quot;PHP 教程&quot;, 
    description: &quot;PHP 是一种创建动态交互性站点的强有力的服务器端脚本语言。&quot;, 
    by: &quot;菜鸟教程&quot;, 
    url: &quot;http://www.runoob.com&quot;, 
    tags: [ &quot;php&quot; ], 
    likes: 200 
}
{  
    title: &quot;Java 教程&quot;, 
    description: &quot;Java 是由Sun Microsystems公司于1995年5月推出的高级程序设计语言。&quot;, 
    by: &quot;菜鸟教程&quot;, 
    url: &quot;http://www.runoob.com&quot;, 
    tags: [ &quot;java&quot; ], 
    likes: 150 
}
{   title: &quot;MongoDB 教程&quot;, 
    description: &quot;MongoDB 是一个 Nosql 数据库&quot;, 
    by: &quot;菜鸟教程&quot;, 
    url: &quot;http://www.runoob.com&quot;, 
    tags: [ &quot;mongodb&quot; ], 
    likes : 100 
}

》》db.getCollection(&apos;teacher&apos;).find({},{&apos;title&apos;:1,_id:0}).sort({&apos;likes&apos;:1})
/* 1 */
{
    &quot;title&quot; : &quot;MongoDB 教程&quot;
}
/* 2 */
{
    &quot;title&quot; : &quot;Java 教程&quot;
}
/* 3 */
{
    &quot;title&quot; : &quot;PHP 教程&quot;
}
</code></pre></li>
</ul>
<p></p><h6>Limit</h6><p></p>
<ul>
<li>需要在MongoDB中读取指定数量的数据记录，可以使用MongoDB的Limit方法</li>
<li>limit()方法接受一个数字参数，该参数指定从MongoDB中读取的记录条数</li>
<li><p>语法为：db.COLLECTION_NAME.find().limit(NUMBER)</p>
<pre><code>》》db.getCollection(&apos;teacher&apos;).find({},{&apos;title&apos;:1,_id:0}).limit(2)
/* 1 */
{
    &quot;title&quot; : &quot;PHP 教程&quot;
}
/* 2 */
{
    &quot;title&quot; : &quot;Java 教程&quot;
}
</code></pre></li>
</ul>
<p></p><h6>Skip</h6><p></p>
<ul>
<li>使用skip()方法来跳过指定数量的数据，skip方法同样接受一个数字参数作为跳过的记录条数</li>
<li><p>语法：db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER)</p>
<pre><code>》》db.getCollection(&apos;teacher&apos;).find({},{&apos;title&apos;:1,_id:0}).limit(1).skip(1)
/* 1 */
{
    &quot;title&quot; : &quot;Java 教程&quot;
}
</code></pre></li>
<li><p>sort(), limilt(),skip() 三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()</p>
</li>
</ul>
</h6></h6>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/22/mongodb排序/" data-id="cjm7bd0p90035s8wvars90ps9" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MongoDB数据库/">MongoDB数据库</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-requests模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/22/requests模块/" class="article-date">
  <time datetime="2018-03-22T07:28:23.000Z" itemprop="datePublished">2018-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/22/requests模块/">requests模块</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>requests模块</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li><p>Requests是用python语言基于urllib编写的，采用的是Apache2 Licensed开源协议的HTTP库，Requests它会比urllib更加方便，可以节约我们大量的工作</p>
<pre><code>import requests
res=requests.get(&apos;https://www.baidu.com&apos;)
print(res.status_code)      #返回状态码
print(res.headers)      #打印请求网址的headers所有信息
print(res.cookies)      #打印请求网址的cookies信息
for key,value in res.cookies.items():
    print(key,&apos;==&apos;,value)
print(res.text)          #res.text返回的是Unicode格式，通常需要转换为utf-8格式，否则就是乱码
res.encoding=&apos;utf-8&apos;
print(res.text)

print(res.content)     #res.content是二进制模式，可下载视频之类的，如想看的话需要decode成utf-8格式
print(res.content.decode(&apos;utf-8&apos;))

#如果你想取文本，可以通过res.text;如果想取图片，文件，则可以通过res.content
</code></pre></li>
</ul>
<p></p><h6>GET请求</h6><p></p>
<ul>
<li><p>带参数的GET请求,如果想查询<a href="http://httpbin.org/get页面的具体参数，需要在url里面加上" target="_blank" rel="noopener">http://httpbin.org/get页面的具体参数，需要在url里面加上</a></p>
<pre><code>import requests
url = &apos;http://httpbin.org/get&apos;
data = {&apos;name&apos;:&apos;zhangsan&apos;,&apos;age&apos;:&apos;25&apos;}
res=requests.get(url,params=data)
print(res.url)                #result:http://httpbin.org/get?age=25&amp;name=zhangsan
res.encoding=&apos;utf-8&apos;
print(res.text)
</code></pre></li>
</ul>
<p></p><h6>Json数据</h6><p></p>
<ul>
<li><p>requests中res.json()方法等同于json.loads（res.text）方法</p>
<pre><code>import requests,json
url = &apos;http://httpbin.org/get&apos;
res=requests.get(url)
print(res.json())
print(json.loads(res.text))
</code></pre></li>
</ul>
<p></p><h6>添加header</h6><p></p>
<ul>
<li><p>添加headers的目的是为了防止发生服务器内部错误，可以正常执行</p>
<pre><code>import requests
url=&apos;https://www.zhihu.com&apos;
header={
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0&quot;
}
res=requests.get(url,headers=header)
res.encoding=&apos;utf-8&apos;
print(res.status_code)
print(res.text)
</code></pre></li>
</ul>
<p></p><h6>post请求</h6><p></p>
<ul>
<li><p>通过post把数据提交到url地址，等同于一字典的形式提交form表单里面的数据</p>
<pre><code>import requests
url=&apos;http://httpbin.org/post&apos;
data = {&apos;name&apos;:&apos;jack&apos;,&apos;age&apos;:&apos;23&apos;}
header={
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0&quot;
}
res=requests.post(url,data=data,headers=header)
res.encoding=&apos;utf-8&apos;
print(res.status_code)
print(res.text)
</code></pre></li>
</ul>
<p></p><h6>高级操作</h6><p></p>
<ul>
<li><p>文件上传</p>
<pre><code>import requests
url=&quot;http://httpbin.org/post&quot;
file={&apos;files&apos;:open(&apos;test.jpg&apos;,&apos;rb&apos;)}
res=requests.post(url,files=file)
res.encoding=&apos;utf-8&apos;
print(res.text)
</code></pre></li>
<li><p>会话维持</p>
<ul>
<li><p>cookie的一个作用就是可以用于模拟登陆，做会话维持</p>
<pre><code>import requests
se = requests.session()     #声明session对象
se.get(&apos;http://httpbin.org/cookies/set/number/12456&apos;) #在一个浏览器中浏览两个页面
res= se.get(&apos;http://httpbin.org/cookies&apos;)
print(res.text)
</code></pre></li>
</ul>
</li>
<li><p>证书验证</p>
<ul>
<li><p>在请求https时，request会进行证书的验证</p>
<pre><code>import requests,urllib3
url=&apos;https://www.12306.cn&apos;
urllib3.disable_warnings()       #消除验证证书的警报
res=requests.get(url,verify=False)
#在请求https时，request会进行证书的验证，如果验证失败则会抛出异常,
#为避免这种情况发生可通过verify=False，但是这样是可以访问到页面结果,也还有警报，可消除
print(res.status_code)
</code></pre></li>
</ul>
</li>
<li><p>代理设置</p>
<ul>
<li><p>受限制网站需要使用代理服务器访问</p>
<pre><code>import requests
proxies = {
    &quot;http&quot;: &quot;http://123.206.75.213:8080&quot;,
}
response = requests.get(&quot;https://www.taobao.com&quot;, proxies=proxies)
print(response.status_code)
</code></pre></li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/22/requests模块/" data-id="cjm7bd0q4003ns8wvqcx5bhl2" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-urllib模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/22/urllib模块/" class="article-date">
  <time datetime="2018-03-21T23:42:42.000Z" itemprop="datePublished">2018-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/22/urllib模块/">urllib模块</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>urllib模块</h5><p></p>
<hr>
<hr>
<p></p><h6>爬虫基本原理</h6><p></p>
<ul>
<li>爬虫是请求网站并提取数据的自动化程序</li>
<li>爬虫的基本流程<ul>
<li>1.发起请求<ul>
<li>通过HTTP库向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应</li>
</ul>
</li>
<li>2.获取响应内容<ul>
<li>如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型</li>
</ul>
</li>
<li>3.解析内容<ul>
<li>得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理</li>
</ul>
</li>
<li>4.保存数据<ul>
<li>保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件</li>
</ul>
</li>
</ul>
</li>
</ul>
<p></p><h6>urllib模块</h6><p></p>
<ul>
<li>Urllib库是Python中的一个功能强大、用于操作URL，并在做爬虫的时候经常要用到的库</li>
<li><p>urllib.request.urlopen(url, data=None,[timeout])</p>
<ul>
<li>可以获取页面，获取页面内容的数据格式为bytes类型，需要进行decode()解码，转换成str类型</li>
<li>url : 需要打开的网址</li>
<li>data : 字典形式，默认为None时是GET方法，data不为空时, * urlopen()的提交方式为POST，注意POST提交时，data需要转换为字节</li>
<li>timeout : 设置网站访问的超时时间</li>
<li><p>urlopen返回对象提供的方法：</p>
<ul>
<li>read(),readline(): 对HTTPResponse类型数据进行操作</li>
<li>info() : 返回HTTPMessage对象，表示远程服务器返回的头信息</li>
<li>getcode() : 返回HTTP状态码</li>
<li><p>geturl(): 返回请求的url</p>
<pre><code>import urllib.request
file=urllib.request.urlopen(&apos;http://www.baidu.com&apos;)
data=file.read()
with open(&apos;./1.html&apos;,&apos;wb&apos;) as f:
    f.write(data)
    f.close()
</code></pre></li>
</ul>
</li>
</ul>
</li>
<li><p>urllib.request.Request(url,data=None,headers={},method=None)</p>
<ul>
<li>有些网页进行了一些反爬虫的设置,我们可以设置一些Headers信息（User-Agent），模拟成浏览器去访问这些网站</li>
<li><p>该函数第一个参数传入url，第二个参数可以传入数据，默认是传入0数据，第三个参数是传入头部，该参数也是有默认值的，默认是不传任何头部；需要创建一个dict，将头部信息以键值对的形式存入到dict对象中，然后将该dict对象传入该函数第三个参数</p>
<pre><code>import urllib.request
url=&apos;http://www.baidu.com&apos;
header={
    &apos;User-Agent&apos;:r&apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0&apos;,
    &apos;Connection&apos;: &apos;keep-alive&apos;
}
request=urllib.request.Request(url,headers=header)
file=urllib.request.urlopen(request,timeout=10)
data=file.read()
with open(&apos;./1.html&apos;,&apos;wb&apos;) as f:
    f.write(data)
    f.close()
</code></pre></li>
</ul>
</li>
<li><p>urllib.parse.urlencode(query, doseq=False,safe=’’,encoding=None,errors=None)</p>
<ul>
<li>主要作用就是将url附上要提交的数据. 对data数据进行编码</li>
<li><p>POST的数据必须是bytes或者iterable of bytes，不能是str，因此需要encode编码</p>
<pre><code>示例一下如何使用爬虫通过POST表单传递信息
测试网址为： http://www.iqianyue.com/mypost 
from urllib import request,parse
url=&apos;http://www.iqianyue.com/mypost&apos;
header={
    &apos;User-Agent&apos;:r&apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0&apos;,
    &apos;Connection&apos;: &apos;keep-alive&apos;
}
data={&apos;name&apos;:&apos;liuyongqian&apos;,&apos;pass&apos;:&apos;123456&apos;}
postdata=parse.urlencode(data).encode(&apos;utf-8&apos;)  #进行编码
req=request.Request(url,postdata,headers=header)
file=request.urlopen(req,timeout=10)
res=file.read()
with open(&apos;./1.html&apos;,&apos;wb&apos;) as f:
    f.write(res)
    f.close()
</code></pre></li>
</ul>
</li>
<li><p>request.ProxyHandler(proxies=None)</p>
<ul>
<li><p>当需要抓取的网站设置了访问限制，这时就需要用到代理来抓取数据；在对方的网站上，显示的不是我们真实的IP地址，而是代理服务器的IP地址</p>
<pre><code>from urllib import request
proxy_hander=request.ProxyHandler({&apos;http&apos;:&apos;5.22.195.215:80&apos;})
opener=request.build_opener(proxy_hander)
res=opener.open(&apos;http://www.baidu.com&apos;)
data=res.read()
with open(&apos;./1.html&apos;,&apos;wb&apos;) as f:
    f.write(data)
    f.close()
</code></pre></li>
</ul>
</li>
<li><p>Cookie的使用</p>
<ul>
<li>进行Cookie处理的一种常用思路如下：<ul>
<li>导入Cookie处理模块http.cookiejar</li>
<li>使用http.cookiejar.CookieJar()创建CookieJar对象</li>
<li>使用HTTPCookieProcessor创建cookie处理器，并以其为参数构建opener对象</li>
<li>创建全局默认的opener对象</li>
</ul>
</li>
<li><p>拿ChinaUnix这个论坛网址就行实战测试一下</p>
<pre><code>from urllib import request,parse,error
import http
from http import cookiejar

url=&apos;http://bbs.chinaunix.net/member.php?mod=logging&amp;action=login&amp;loginsubmit=yes&amp;loginhash=LpozE&apos;
data={
    &apos;username&apos;:&apos;myspiders&apos;,
    &apos;password&apos;:&apos;39a39b39c39#&apos;
}
postdata=parse.urlencode(data).encode(&apos;utf-8&apos;)
header={
    &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.221 Safari/537.36 SE 2.X MetaSr 1.0&apos;,
    &apos;Referer&apos;:&apos;http://bbs.chinaunix.net/member.php?mod=logging&amp;action=login&amp;logsubmit=yes&apos;
}
req=request.Request(url,postdata,headers=header)
cjar=http.cookiejar.CookieJar()  #创建cookiejar对象
cookie=request.HTTPCookieProcessor(cjar)  #创建cookie处理器，并以其为参数构建opener对象
opener=request.build_opener(cookie)
request.install_opener(opener)  #将opener安装为全局
try:
    res=request.urlopen(req)
except error.HTTPError as e:
    print(e.code)
    print(e.reason)
with open(&apos;./test1.html&apos;,&apos;wb&apos;) as f:  #保存登录后的页面到文件
    f.write(res.read())
    f.close()
url2=&apos;http://bbs.chinaunix.net/forum-326-1.html&apos;  #再次访问该网站的其他的页面
res2=request.urlopen(url2)
with open(&apos;./test2.html&apos;,&apos;wb&apos;) as f2:
    f2.write(res2.read())
    f2.close()
</code></pre></li>
<li><p>参考<a href="http://blog.csdn.net/fengxinlinux/article/details/77340666" target="_blank" rel="noopener"> Python3爬虫代理服务器与cookie的使用</a></p>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/22/urllib模块/" data-id="cjm7bd0qz0044s8wvtq265bgk" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/爬虫学习/">爬虫学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-Django基本概念" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/22/Django基本概念/" class="article-date">
  <time datetime="2018-03-21T16:00:42.000Z" itemprop="datePublished">2018-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/22/Django基本概念/">Django基本概念</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>Django基本概念</h5><p></p>
<hr>
<hr>
<p></p><h6>简介</h6><p></p>
<ul>
<li>Django是一个开放源代码的Web应用框架，由Python写成。采用了MTV的框架模式，即模型M，模板T和视图V</li>
<li>Django 框架的核心组件有：<ul>
<li>用于创建模型的对象关系映射</li>
<li>为最终用户设计的完美管理界面</li>
<li>一流的 URL 设计</li>
<li>设计者友好的模板语言</li>
<li>缓存系统</li>
</ul>
</li>
<li>Django是一个基于MVC构造的框架。但是在Django中，控制器接受用户输入的部分由框架自行处理，所以 Django 里更关注的是模型（Model）、模板(Template)和视图（Views），称为 MTV模式。它们各自的职责如下：<ul>
<li>模型（Model）即数据存取层<ul>
<li>处理与数据相关的所有事务： 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等</li>
</ul>
</li>
<li>视图（View）即表现层<ul>
<li>处理与表现相关的决定： 如何在页面或其他类型文档中进行显示</li>
</ul>
</li>
<li>模板(Template)即业务逻辑层<ul>
<li>存取模型及调取恰当模板的相关逻辑。模型与模板的桥梁</li>
</ul>
</li>
</ul>
</li>
<li><p>c/s与b/s的区别</p>
<pre><code>c/s--&gt;client/server 客户端与服务器交互
    主要工作在客户端 处理请求  响应请求
    好：分减服务器压力 用户体验更好，方便
    坏：下载客户端
b/s--&gt;browser/server 浏览器与服务器交互
    主要工作在服务器 处理请求 相应请求
    好：不需要下载客户端
    坏：大量的工作精力花销子啊服务端
</code></pre></li>
<li><p>目录介绍</p>
<pre><code>文件夹dj_myschool
    文件夹dj_myschool
        __init__.py     #一个空文件，告诉python该目录是一个python包
        settings.py     #该项目的设置与配置
        urls.py         #该项目的URL声明，一份由Django驱动的网站目录
        wsgi.py            #一个WSGI兼容的web服务器的入口，以便运行你的项目
    文件夹student
        文件夹migrations
            __init__.py
        __init__.py
        admin.py
        apps.py
        models.py      #负责业务对象和数据库的关系映射(ORM)
        tests.py
        views.py       #负责业务逻辑并在适当时候调用Models和Templates
        文件夹
    文件夹templates    #负责如何把页面展示给用户(html)
    文件db.sqlite3
    文件manage.py          #一个实用的命令行工具
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/22/Django基本概念/" data-id="cjm7bd0lc000cs8wvptf3ew4b" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Django学习/">Django学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-MongoDB基本操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/19/MongoDB基本操作/" class="article-date">
  <time datetime="2018-03-19T12:10:50.000Z" itemprop="datePublished">2018-03-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/19/MongoDB基本操作/">MongoDB基本操作</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>MongoDB基本操作</h5><p></p>
<hr>
<hr>
<p></p><h6>01</h6><p></p>
<ul>
<li><p>1.创建数据库</p>
<pre><code>use db_name   #如果数据库不存在，则创建数据库，否则切换到指定数据库
db            #查看当前数据库名
show dbs      #查看所有数据库；刚创建的数据库不会在数据库列表中，要显示的话，需要插入数据
              # mongodb中默认的数据库为test，如没有创建新的数据库，集合将存放在test数据库中
</code></pre></li>
<li><p>2.删除数据库</p>
<pre><code>db.dropDatabase()   #删除当前数据库
</code></pre></li>
<li><p>3.创建集合</p>
<ul>
<li>db.createCollection(name,option) 其中name为集合名称，option为可选参数，指定有关内存大小及索引的选项</li>
<li><p>options 可以是如下参数：</p>
<ul>
<li>capped    如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档<br>当该值为 true 时，必须指定 size 参数</li>
<li>autoIndexId  如为 true，自动在 _id 字段创建索引。默认为 false</li>
<li>size   为固定集合指定一个最大值（以字节计），如果 capped 为 true，也需要指定该字段</li>
<li><p>max    指定固定集合中包含文档的最大数量</p>
<pre><code>use myfirstdb             #创建数据库
db.createCollection(&apos;myinfo&apos;)   #创建集合
show collections        #查看已有集合

#下面是带有几个关键参数的 createCollection() 的用法：
创建固定集合 myinfo，整个集合空间大小 6142800 KB, 文档最大个数为 10000 个
db.createCollection(&apos;myinfo&apos;,{capped:true,autoIndexId:true,size:6142800,max:10000})
</code></pre></li>
</ul>
</li>
</ul>
</li>
<li><p>4.删除集合</p>
<ul>
<li><p>db.collection_name.drop()如果删除成功，返回true</p>
<pre><code>db.myinfo.drop()     #result:true
</code></pre></li>
</ul>
</li>
</ul>
<p></p><h6>02</h6><p></p>
<ul>
<li><p>1.插入文档</p>
<ul>
<li>文档的数据结构和JSON基本一样,所有存储在集合中的数据都是BSON格式</li>
<li><p>BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON</p>
<pre><code>db.collection_name.insert(键值对，逗号隔开)
db.collection_name.find()      #查看已经插入文档

#可以将数据定义为一个变量，如下所示
doc1=({
    description:&apos;this is my pravite infomation&apos;,
    name:&apos;liuyongqian&apos;,
    age:108,
    work:&apos;Engineer&apos;,
    url:&apos;http://www.liuyongqian.com&apos;
})
db.myinfo.insert(doc1)
</code></pre></li>
</ul>
</li>
<li><p>2.更新文档</p>
<ul>
<li><p>update()用于更新已经存在的文档</p>
<pre><code>db.collection_name.update(
    &lt;query&gt;,#查询条件，类似sql update查询内where后面的
    &lt;update&gt;,#更新内容，可以理解为sql update查询内set后面的
    {
        upsert:&lt;boolean&gt;, #可选，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入
        multi:&lt;boolean&gt;, #可选，默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新
        writeConcern:&lt;document&gt;  #可选，抛出异常的级别
    }
)
#如更新‘work’
db.myinfo.update(
    {&apos;work&apos;:&apos;Engineer&apos;},
    {$set:{&apos;work&apos;:&apos;Farmer&apos;}},
    {multi:true}
)
</code></pre></li>
<li><p>save()通过传入的文档来替换已有文档</p>
<pre><code>db.collection_name.save(
    &lt;document&gt;,#文档数据
    {
        writeConcern:&lt;document&gt;
    }
)

#如更新&apos;work&apos;
db.myinfo.save(
    {&apos;work&apos;:&apos;Father&apos;}
)
</code></pre></li>
</ul>
</li>
<li><p>删除文档</p>
<ul>
<li><p>官方推荐使用 deleteOne() 和 deleteMany() 方法</p>
<pre><code>#如删除集合下全部文档
db.myinfo.deleteMany({})
#如删除status等于A的全部文档
db.myinfo.deleteMany({status:&apos;A&apos;})
#如删除status等于B的全部文档
db.myinfo.deleteOne({status:&apos;B&apos;})
</code></pre></li>
</ul>
</li>
<li><p>查询文档<br>find() 方法以非结构化的方式来显示所有文档</p>
<pre><code>db.collection_name.find(query,projection)
#query ：可选，使用查询操作符指定查询条件
#projection ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）

db.collection_name.find().pretty()#以格式化的方式来显示所有文档

db.col.find({},{title:1,_id:0}).limit(2)  第一个大括号的意思是，要把查询出来的结果以Bson的形式展现出来；第二个大括号的意思是，要查询的字段有哪些，属性后面的1或者0代表的意思是查询和不查询的意思，那么不想查询_id的值，为何多此一举的写句_id:0？原因是这样的，在查询的时候，如果只写title:1，那么会默认的把_id也查询出来，为了过滤掉_id字段，我们需要加上_id:0。当然了，如果你还想查询更多的字段就在第二个{}里面加就可以了
</code></pre></li>
<li><p>AND条件， find() 方法可以传入多个键(key)，每个键(key)以逗号隔开</p>
<pre><code>db.clooection_name.find(key1:value1,key2:value2)
#如通过name、age键来查询数据
db.myinfo.find({&apos;name&apos;:&apos;liuyongqian&apos;,&apos;age&apos;:&apos;108&apos;})
</code></pre></li>
</ul>
<ul>
<li><p>OR条件，使用了关键字<code>$or</code></p>
<pre><code>db.collection_name.find({$or:[{key1:value1},{key2:value2}]})
#如通过name或者age键来查询数据
db.myinfo.find({$or:[{&apos;name&apos;:&apos;liuyongqian&apos;},{&apos;age&apos;:&apos;108&apos;}]})
</code></pre></li>
<li><p>比较符</p>
<pre><code>等于 {key:value}  db.myinfo.find({&apos;age&apos;:&apos;108&apos;})    where age=108
小于 {key:{$lt:value}}  db.myinfo.find({&apos;age&apos;:{$lt:108}})   where age&lt;108
大于 {key:{$gt:value}}  db.myibfo.find({&apos;age&apos;:{$gt:108}})   where age&gt;108
小于等于 {key:{$lte:value}}  db.myinfo.find({&apos;age&apos;:{$lte:108}})  where age&lt;=108
大于等于 {key:{$gte:value}}  db.myinfo.find({&apos;age&apos;:{$gte:108}})  where age&gt;=108
不等于   {key:{$ne:value}} db.myinfo.find({&apos;age&apos;:{$ne:108}})  where age!=108
</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/19/MongoDB基本操作/" data-id="cjm7bd0n3001hs8wvpf9jdllb" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MongoDB数据库/">MongoDB数据库</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-Python之socket" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/19/Python之socket/" class="article-date">
  <time datetime="2018-03-19T12:10:17.000Z" itemprop="datePublished">2018-03-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/19/Python之socket/">Python之socket</a>
    </h1>
  


      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
	  
	  
        <p></p><h5>Python之socket</h5><p></p>
<hr>
<hr>
<p></p><h6>socket是什么</h6><p></p>
<ul>
<li><p>OSI（开放式系统互联模型）</p>
<ul>
<li><p>互联网协议按照功能不同分为osi七层或者tcp/ip四层</p>
<pre><code>应用层----C/S架构的软件是基于网络进行通讯的；学习socket就是为了完成C/S架构的开发
传输层
网络层
网络接口层
</code></pre></li>
</ul>
</li>
<li><p>Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议</p>
</li>
<li><p>扫盲篇</p>
<pre><code>1 将socket说成ip+port，ip是用来标识互联网中的一台主机的位置，而port是用来标识这台机器上的一个应用程序，ip地址是配置到网卡上的，而port是应用程序开启的，ip与port的绑定就标识了互联网中独一无二的一个应用程序
2 
3 而程序的pid是同一台机器上不同进程或者线程的标识（Google Chrome会有多个PID）
</code></pre></li>
</ul>
<p></p><h6>基于TCP的套接字</h6><p></p>
<ul>
<li><p>tcpserver.py</p>
<pre><code>import socket
server=socket.socket(socket.AF_INET,socket.SOCK_STREAM)#获取tcp/ip套接字
ip_port=(&apos;127.0.0.1&apos;,8001)
server.bind(ip_port)#绑定(主机,端口号)到套接字
server.listen(5)    #开始TCP监听,5的作用是最大挂起连接数
print(&apos;-------服务端开始运行--------&apos;)
conn,addr=server.accept()#被动接受TCP客户的连接,(阻塞式)等待连接的到来
#print(&apos;双向连接是：&apos;,conn)
#print(&apos;客户端地址：：&apos;,addr)
while True: #通讯循环
    msg=conn.recv(1024)#接收tcp信息
    print(&apos;客户端发来的消息：&apos;,msg.decode(&apos;utf-8&apos;))
    conn.send(input(&apos;请服务端回复：&apos;).encode(&apos;utf-8&apos;))#发送tcp消息
conn.close()#关闭客户端套接字
server.close()#关闭服务器套接字
</code></pre></li>
<li><p>tcpclient.py</p>
<pre><code>import socket
client=socket.socket(socket.AF_INET,socket.SOCK_STREAM) #创建客户端套接字
ip_port=(&apos;127.0.0.1&apos;,8001)
client.connect(ip_port)     #主动初始化TCP服务器连接
while True:
    client.send(input(&apos;请输入客户端信息：&apos;).encode(&apos;utf-8&apos;)) #发送信息
    data=client.recv(1024)  ##收到服务器发来的消息
    print(&apos;服务端返回的消息：&apos;,data.decode(&apos;utf-8&apos;))
client.close()   #关闭客户端套接字
</code></pre></li>
</ul>
<p></p><h6>基于UDP的套接字</h6><p></p>
<ul>
<li><p>udpserver.py</p>
<pre><code>import socket
server=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)#创建服务器的套接字
ipport=(&apos;127.0.0.1&apos;,8001)
server.bind(ipport)#绑定服务器套接字
while True:
    data,addr=server.recvfrom(1024)
    print(&apos;收到：&apos;,str(addr),&apos;消息：&apos;,data.decode(&apos;utf-8&apos;))
    msg=input(&apos;请回复:&apos;).encode(&apos;utf-8&apos;)
    server.sendto(msg,addr)
</code></pre></li>
<li><p>udpclient.py</p>
<pre><code>import socket
client=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)
ip_port=(&apos;127.0.0.1&apos;,8001)
while True:
    msg=input(&apos;请输入发送内容:&apos;).encode(&apos;utf-8&apos;)
    client.sendto(msg,ip_port)  #数据，ip地址+端口
    data,addr=client.recvfrom(1024)
    print(&apos;收到{}返回消息&apos;.format(addr),data.decode(&apos;utf-8&apos;))
</code></pre></li>
</ul>
<p><em>小白参考大神佳作<a href="https://www.cnblogs.com/nulige/p/6235531.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">python网络编程-socket编程</a></em><br><em>若文中内容有误，希望大家指正，谢谢!</em></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/19/Python之socket/" data-id="cjm7bd0ny001ys8wv379zo10b" class="article-share-link">Teilen</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python学习/">Python学习</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; zurück</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/5/">weiter &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django学习/">Django学习</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git学习/">Git学习</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTTP协议/">HTTP协议</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux学习/">Linux学习</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MongoDB数据库/">MongoDB数据库</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL学习/">MySQL学习</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python学习/">Python学习</a><span class="tag-list-count">27</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis数据库/">Redis数据库</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web前端/">Web前端</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/安全测试/">安全测试</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/性能测试/">性能测试</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫学习/">爬虫学习</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/自动化测试/">自动化测试</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/软件工具/">软件工具</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Django学习/" style="font-size: 13.75px;">Django学习</a> <a href="/tags/Git学习/" style="font-size: 13.75px;">Git学习</a> <a href="/tags/HTTP协议/" style="font-size: 10px;">HTTP协议</a> <a href="/tags/Linux学习/" style="font-size: 18.75px;">Linux学习</a> <a href="/tags/MongoDB数据库/" style="font-size: 12.5px;">MongoDB数据库</a> <a href="/tags/MySQL学习/" style="font-size: 15px;">MySQL学习</a> <a href="/tags/Python学习/" style="font-size: 20px;">Python学习</a> <a href="/tags/Redis数据库/" style="font-size: 16.25px;">Redis数据库</a> <a href="/tags/Web前端/" style="font-size: 17.5px;">Web前端</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/安全测试/" style="font-size: 11.25px;">安全测试</a> <a href="/tags/性能测试/" style="font-size: 11.25px;">性能测试</a> <a href="/tags/爬虫学习/" style="font-size: 18.75px;">爬虫学习</a> <a href="/tags/自动化测试/" style="font-size: 13.75px;">自动化测试</a> <a href="/tags/软件工具/" style="font-size: 11.25px;">软件工具</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">18</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">46</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">11</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/09/18/nmon工具的使用/">nmon工具的使用</a>
          </li>
        
          <li>
            <a href="/2018/09/11/破解IDEA2018-1/">破解IDEA2018.1</a>
          </li>
        
          <li>
            <a href="/2018/08/17/Redis安装与部署/">Redis安装与部署</a>
          </li>
        
          <li>
            <a href="/2018/08/04/JAVA项目更新部署/">JAVA项目更新部署</a>
          </li>
        
          <li>
            <a href="/2018/07/13/Linux上部署java项目/">Linux上部署java项目</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">About</h3>
    <div class="widget">
       Email:<a>liuyongqian51@163.com</a><br />
          QQ:<a>272501447</a><br />
	  Github:<a></a>
    </div>
  </div>

  
  

</aside>


        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 刘永前<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>

</footer>


    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>